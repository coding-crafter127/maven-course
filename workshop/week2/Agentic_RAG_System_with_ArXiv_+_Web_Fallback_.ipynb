{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3sQoe2Tg_fb"
      },
      "source": [
        "# ðŸ“š Agentic RAG System with ArXiv + Web Fallback\n",
        "\n",
        "This project implements an **intelligent research assistant** that retrieves and synthesizes information using:\n",
        "1. **ArXiv papers** as the primary knowledge source (**RAG approach**)\n",
        "2. **Web search (Tavily API)** as a fallback mechanism\n",
        "3. **LangGraph** for orchestrating the decision-making workflow\n",
        "\n",
        "## ðŸŽ¯ Purpose\n",
        "\n",
        "The system is designed to provide high-quality, research-backed answers to technical and scientific questions by:\n",
        "- Prioritizing academic and research papers from ArXiv for scientific queries\n",
        "- Falling back to web search for recent developments or non-academic topics\n",
        "- Maintaining conversation context for coherent multi-turn interactions\n",
        "- Ensuring proper attribution and citations in responses\n",
        "\n",
        "## ðŸ”‘ Prerequisites\n",
        "\n",
        "To use this system, you'll need:\n",
        "\n",
        "1. **OpenAI API Key**\n",
        "   - Required for:\n",
        "     - Text embeddings (for semantic search)\n",
        "     - Response generation (GPT-4 Turbo)\n",
        "     - Routing decisions (GPT-3.5 Turbo)\n",
        "   - Get it from: [OpenAI Platform](https://platform.openai.com)\n",
        "\n",
        "2. **Tavily API Key**\n",
        "   - Required for:\n",
        "     - Web search fallback functionality\n",
        "     - Real-time information retrieval\n",
        "     - Academic domain filtering\n",
        "   - Get it from: [Tavily](https://app.tavily.com)\n",
        "\n",
        "3. **Python Environment**\n",
        "   - Python 3.8 or higher\n",
        "   - Required packages (will be installed automatically):\n",
        "     - langchain-community\n",
        "     - langchain_chroma\n",
        "     - langchain_core\n",
        "     - langchain_openai\n",
        "     - langchain_text_splitters\n",
        "     - langgraph\n",
        "     - tavily-python\n",
        "     - openai\n",
        "     - python-dotenv\n",
        "\n",
        "\n",
        "## ðŸ¤– Agentic Workflow Architecture\n",
        "\n",
        "The user workflow is translated into an agentic system through the following components:\n",
        "\n",
        "1. **State Management**\n",
        "   - **Conversation State**: Tracks user queries, system responses, and context\n",
        "   - **Search State**: Maintains information about current search results and sources\n",
        "   - **Decision State**: Stores routing decisions and their rationale\n",
        "\n",
        "2. **Agent Components**\n",
        "   - **Router Agent**: Makes intelligent decisions about information sources\n",
        "     - Analyzes query type and context\n",
        "     - Determines optimal search strategy\n",
        "     - Handles fallback mechanisms\n",
        "   \n",
        "   - **Search Agent**: Executes information retrieval\n",
        "     - Manages ArXiv API interactions\n",
        "     - Handles Tavily web search\n",
        "     - Processes and filters results\n",
        "   \n",
        "   - **Synthesis Agent**: Combines and formats information\n",
        "     - Merges multiple sources\n",
        "     - Ensures proper attribution\n",
        "     - Generates coherent responses\n",
        "\n",
        "3. **Feedback Loop**\n",
        "   - System learns from user interactions\n",
        "   - Improves routing decisions over time\n",
        "   - Adapts to user preferences and query patterns\n",
        "\n",
        "## ðŸ“Š Data Requirements and Sources\n",
        "\n",
        "The system requires and manages several types of data:\n",
        "\n",
        "1. **Input Data**\n",
        "   - **User Queries**: Natural language questions and follow-ups\n",
        "   - **Conversation History**: Previous interactions for context\n",
        "   - **User Preferences**: Optional settings for search behavior\n",
        "\n",
        "2. **Knowledge Sources**\n",
        "   - **ArXiv Papers**:\n",
        "     - Source: ArXiv API\n",
        "     - Format: PDF documents\n",
        "     - Update Frequency: Daily\n",
        "     - Coverage: Scientific and technical papers\n",
        "   \n",
        "   - **Web Content**:\n",
        "     - Source: Tavily API\n",
        "     - Format: Web pages and documents\n",
        "     - Update Frequency: Real-time\n",
        "     - Coverage: News, blogs, documentation, etc.\n",
        "\n",
        "3. **Processed Data**\n",
        "   - **Embeddings**: Vector representations of text\n",
        "     - Generated using OpenAI's embedding model\n",
        "     - Stored in vector database\n",
        "   \n",
        "   - **Chunks**: Processed text segments\n",
        "     - Size: Optimized for semantic search\n",
        "     - Metadata: Source, date, relevance score\n",
        "   \n",
        "   - **Citations**: Reference information\n",
        "     - Paper titles, authors, URLs\n",
        "     - Web page sources and dates\n",
        "\n",
        "4. **Output Data**\n",
        "   - **Responses**: Generated answers with citations\n",
        "   - **Search Results**: Ranked and filtered information\n",
        "   - **Conversation Logs**: Interaction history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0zyWACuhcfqG",
        "outputId": "8afe1065-de41-4dad-e5e9-63aeae3f2334"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Install required packages\n",
        "! pip install -qU langchain langgraph pypdf chromadb tavily-python openai python-dotenv pyboxen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ai9xIIrecwKb",
        "outputId": "c48c9116-3975-48d9-f85e-0279df722886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.3.21)\n",
            "Requirement already satisfied: langchain_chroma in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.2.3)\n",
            "Requirement already satisfied: langchain_core in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.3.52)\n",
            "Requirement already satisfied: langchain_openai in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.3.13)\n",
            "Requirement already satisfied: langchain_text_splitters in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.3.8)\n",
            "Requirement already satisfied: langgraph in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.3.30)\n",
            "Requirement already satisfied: tavily-python in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (0.5.4)\n",
            "Requirement already satisfied: openai in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (1.75.0)\n",
            "Requirement already satisfied: python-dotenv in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (3.11.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain-community) (2.2.4)\n",
            "Requirement already satisfied: chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_chroma) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_core) (2.11.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: build>=1.0.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.34.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.25.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.16)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (14.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from httpx->tavily-python) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from httpx->tavily-python) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: pyproject_hooks in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.45.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.39.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
            "Requirement already satisfied: sympy in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/jayant/extra_disk/projects/maven-course/.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community langchain_chroma langchain_core langchain_openai langchain_text_splitters langgraph tavily-python openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDJAvAUpcjmN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Import required libraries\n",
        "import os  # Provides functions to interact with the operating system.\n",
        "from pyboxen import boxen  # Used to display stylized boxes in the terminal for better CLI UI.\n",
        "from getpass import getpass  # Allows secure password input without echoing.\n",
        "from typing import TypedDict, List, Dict, Optional, Literal, Union, Annotated, cast  # Used for type annotations and static type checking.\n",
        "from langchain_core.documents import Document  # Represents and structures text data in LangChain.\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses raw LLM output into usable string format.\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Loads and extracts text from PDF documents.\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter  # Splits text into chunks using markdown headers or character limits.\n",
        "from langchain_chroma import Chroma  # Provides integration with Chroma vector store for embedding storage and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Interfaces with OpenAI for embeddings and chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Manages prompt templates for chat-based interactions.\n",
        "from langgraph.graph import StateGraph, END  # Helps define state-based logic flows for chat systems.\n",
        "from tavily import TavilyClient  # Interfaces with Tavily for real-time web search.\n",
        "from langchain.memory import ConversationBufferMemory  # Maintains memory of past conversation for context retention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOl5svrxoDkQ"
      },
      "source": [
        "# API Key Submission\n",
        "\n",
        "Please follow the instructions below:\n",
        "\n",
        "1. **Provide the Tavily API Key**\n",
        "2. **Provide the Open API Key**\n",
        "3. **Press Enter** to proceed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1CrD7yecpRF",
        "outputId": "71b50172-b442-4f98-a3c4-026de430841e"
      },
      "outputs": [],
      "source": [
        "# Set API keys\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter Tavily API Key (get from https://app.tavily.com): \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsoCt4BhfRp"
      },
      "source": [
        "## 2. Define State and System Architecture\n",
        "\n",
        "This is important because Agents need context to take decisions and showcase \"Agency\", the state helps us define the information that the agent will require and also capture information through out the whole process.\n",
        "\n",
        "We'll define our system's state and flow using **LangGraph**. The state will track our:\n",
        "- **Input question**\n",
        "- **Retrieved ArXiv results**\n",
        "- **Web search results**\n",
        "- **Final answer**\n",
        "- **Conversation history for context**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "9DX0VBugddBN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Define our system state - this is what passes between nodes in our graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State definition for our agentic RAG system\"\"\"\n",
        "    question: str  # User's current question\n",
        "    arxiv_results: Optional[List[Document]]  # Results from ArXiv papers (if any)\n",
        "    web_results: Optional[List[Dict]]  # Results from web search (if any)\n",
        "    answer: str  # Final synthesized answer\n",
        "    conversation_history: str  # Previous Q&A for context\n",
        "    memory: any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym7UTPc4hpli"
      },
      "source": [
        "## 3. Router Node Implementation\n",
        "\n",
        "The **Router Node** is responsible for deciding whether to use **ArXiv papers** or **web search**.\n",
        "- **First**, it tries to use **ArXiv papers** (our local knowledge source).\n",
        "- **Falls back** to **web search** if needed.\n",
        "\n",
        "This demonstrates **strategic decision-making capabilities**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4oSbkgqwdh_R"
      },
      "outputs": [],
      "source": [
        "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a highly specialized research assistant with access to two information sources:\n",
        "1. A collection of ArXiv research papers\n",
        "2. A web search tool\n",
        "\n",
        "Your task is to determine which source would be better to answer the user's question.\n",
        "FIRST try to use ArXiv papers for scientific and academic questions.\n",
        "ONLY use web search if:\n",
        "- The question requires very recent information not likely in research papers\n",
        "- The question is about general knowledge, news, or non-academic topics\n",
        "- The question asks for information beyond what academic papers would contain\n",
        "\n",
        "Consider the conversation history for context.\n",
        "\n",
        "Question: {question}\n",
        "Conversation History: {conversation_history}\n",
        "\n",
        "Respond with ONLY ONE of these two options:\n",
        "\"arxiv\" - if the question should be answered using research papers\n",
        "\"web\" - if the question requires web search\n",
        "\n",
        "Your decision should be a single word only (either \"arxiv\" or \"web\"). Do not include any explanation, reasoning, or additional text in your response.\n",
        "\"\"\")\n",
        "\n",
        "def router_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Determines whether to use ArXiv papers or web search based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question and conversation history\n",
        "\n",
        "    Returns:\n",
        "        Dict indicating which path to take next\n",
        "    \"\"\"\n",
        "    # Use a lighter model for routing decisions\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "    # Create a chain that outputs just the decision text\n",
        "    chain = router_prompt | llm\n",
        "\n",
        "    # Invoke the chain with our question and history\n",
        "    # Get the content of the AIMessage object instead of directly calling strip()\n",
        "    decision = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    }).content.strip().lower()\n",
        "\n",
        "    print(f\"Router decision: {decision}\")\n",
        "\n",
        "    # Return the next node to be called based on the decision\n",
        "    if \"web\" in decision:\n",
        "        return {\"next\": \"web_search\"}\n",
        "    else:\n",
        "        return {\"next\": \"arxiv_retrieval\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D_WGWpKhz4D"
      },
      "source": [
        "# ArXiv Processor Documentation\n",
        "\n",
        "## Overview\n",
        "The `ArXivProcessor` class is designed to handle processing ArXiv PDFs for retrieval-augmented generation (RAG) systems. It implements document-aware chunking strategies specifically optimized for scientific papers.\n",
        "\n",
        "## Key Features\n",
        "- **Two-step chunking strategy**:\n",
        " 1. Markdown header splitting to preserve document structure\n",
        " 2. Recursive character splitting for handling longer sections effectively\n",
        "- **Confidence-based retrieval** with threshold filtering\n",
        "- **Metadata preservation** from original PDFs\n",
        "\n",
        "## Class Structure\n",
        "\n",
        "### Constructor: `__init__()`\n",
        "Initializes the processor with specialized document chunking strategies:\n",
        "- `MarkdownHeaderTextSplitter` to maintain document section structure\n",
        "- `RecursiveCharacterTextSplitter` for detailed content subdivision\n",
        "\n",
        "### Methods\n",
        "\n",
        "#### `load_and_process(pdf_urls: List[str])`\n",
        "Processes ArXiv PDFs with document-aware chunking:\n",
        "- Loads PDFs from provided URLs\n",
        "- Converts content to markdown-style text with headers\n",
        "- Applies two-stage chunking process\n",
        "- Creates a vector store with OpenAI embeddings\n",
        "\n",
        "#### `retrieve(question: str, confidence_threshold: float = 0.75, k: int = 5)`\n",
        "Retrieves relevant chunks with confidence scoring:\n",
        "- Performs similarity search based on user query\n",
        "- Filters results by confidence threshold\n",
        "- Returns only high-relevance document chunks\n",
        "\n",
        "## Implementation Example\n",
        "The documented code includes a sample implementation that loads and processes two ArXiv papers:\n",
        "- Quantum computing paper: https://arxiv.org/pdf/2305.10343.pdf\n",
        "- LLM research paper: https://arxiv.org/pdf/2303.04137.pdf\n",
        "\n",
        "## Dependencies\n",
        "- `PyPDFLoader` for PDF handling\n",
        "- `MarkdownHeaderTextSplitter` and `RecursiveCharacterTextSplitter` for content chunking\n",
        "- `OpenAIEmbeddings` for vector embeddings\n",
        "- `Chroma` for vector storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "QeTPjOKgdom7",
        "outputId": "98971eb4-0ba0-42c1-b33c-88408d7211b4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36mâ•­â”€\u001b[0m\u001b[36m >>> Initialization \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m                                                         \n",
            "\u001b[36mâ”‚\u001b[0m                                                        \u001b[36mâ”‚\u001b[0m                                                         \n",
            "\u001b[36mâ”‚\u001b[0m   Initializing ArXiv processor with sample papers...   \u001b[36mâ”‚\u001b[0m                                                         \n",
            "\u001b[36mâ”‚\u001b[0m                                                        \u001b[36mâ”‚\u001b[0m                                                         \n",
            "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                         \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mâ•­â”€\u001b[0m\u001b[34m >>> PDF Loading \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m                                                          \n",
            "\u001b[34mâ”‚\u001b[0m                                                       \u001b[34mâ”‚\u001b[0m                                                          \n",
            "\u001b[34mâ”‚\u001b[0m   Loading PDF from https://arxiv.org/pdf/2504.10412   \u001b[34mâ”‚\u001b[0m                                                          \n",
            "\u001b[34mâ”‚\u001b[0m                                                       \u001b[34mâ”‚\u001b[0m                                                          \n",
            "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                          \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mâ•­â”€\u001b[0m\u001b[32m >>> Processing Complete \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m                                                                              \n",
            "\u001b[32mâ”‚\u001b[0m                                   \u001b[32mâ”‚\u001b[0m                                                                              \n",
            "\u001b[32mâ”‚\u001b[0m   Created 35 chunks from 1 PDFs   \u001b[32mâ”‚\u001b[0m                                                                              \n",
            "\u001b[32mâ”‚\u001b[0m                                   \u001b[32mâ”‚\u001b[0m                                                                              \n",
            "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                              \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mâ•­â”€\u001b[0m\u001b[32m >>> Status \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m                                                                               \n",
            "\u001b[32mâ”‚\u001b[0m                                  \u001b[32mâ”‚\u001b[0m                                                                               \n",
            "\u001b[32mâ”‚\u001b[0m   ArXiv processor initialized!   \u001b[32mâ”‚\u001b[0m                                                                               \n",
            "\u001b[32mâ”‚\u001b[0m                                  \u001b[32mâ”‚\u001b[0m                                                                               \n",
            "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                               \n",
            "\n"
          ]
        }
      ],
      "source": [
        "class ArXivProcessor:\n",
        "    \"\"\"\n",
        "    Handles processing ArXiv PDFs for retrieval-augmented generation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the processor with document-aware chunking strategies.\n",
        "\n",
        "        The chunking strategy uses a two-step approach:\n",
        "        1. Markdown header splitting preserves document structure and headers\n",
        "        2. Recursive character splitting handles longer sections effectively\n",
        "        \"\"\"\n",
        "        # Header splitter preserves section structure in scientific papers\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"Section\"),           # Main sections\n",
        "                (\"##\", \"Subsection\"),       # Subsections\n",
        "                (\"###\", \"Subsubsection\")    # Sub-subsections\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Recursive splitter handles nested hierarchies and technical content\n",
        "        # - Chunk size of 1000 balances context vs specificity\n",
        "        # - Overlap of 200 ensures continuity between chunks\n",
        "        # - Separators prioritize natural breaks in scientific text\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Will be initialized when documents are loaded\n",
        "        self.vector_store = None\n",
        "\n",
        "    def load_and_process(self, pdf_urls: List[str]):\n",
        "        \"\"\"\n",
        "        Process ArXiv PDFs with document-aware chunking\n",
        "\n",
        "        Args:\n",
        "            pdf_urls: List of URLs to ArXiv PDFs\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        # Process each PDF\n",
        "        for url in pdf_urls:\n",
        "            print(boxen(f\"Loading PDF from {url}\", title=\">>> PDF Loading\", color=\"blue\", padding=1))\n",
        "            loader = PyPDFLoader(url)\n",
        "            pages = loader.load()\n",
        "\n",
        "            # Process each page\n",
        "            for page in pages:\n",
        "                # Convert PDF content to markdown-style text with headers\n",
        "                page_text = f\"# {page.metadata['source']}\\n## Page {page.metadata['page']}\\n{page.page_content}\"\n",
        "\n",
        "                # First split by headers to maintain document structure\n",
        "                header_chunks = self.header_splitter.split_text(page_text)\n",
        "\n",
        "                # Then split large sections into smaller chunks\n",
        "                small_chunks = self.text_splitter.split_documents(header_chunks)\n",
        "\n",
        "                # Add to our collection\n",
        "                all_chunks.extend(small_chunks)\n",
        "\n",
        "        print(boxen(f\"Created {len(all_chunks)} chunks from {len(pdf_urls)} PDFs\", title=\">>> Processing Complete\", color=\"green\", padding=1))\n",
        "\n",
        "        # Create vector store with OpenAI embeddings\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=all_chunks,\n",
        "            embedding=OpenAIEmbeddings(),\n",
        "            persist_directory=\"./arxiv_db\"\n",
        "        )\n",
        "\n",
        "    def retrieve(self, question: str, confidence_threshold: float = 0.75, k: int = 5):\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with confidence scoring\n",
        "\n",
        "        Args:\n",
        "            question: User question to find relevant information for\n",
        "            confidence_threshold: Minimum relevance score (0-1) to include a result\n",
        "            k: Maximum number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant document chunks that meet the threshold\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"No ArXiv documents loaded. Run load_and_process first.\")\n",
        "\n",
        "        # Perform similarity search with relevance scores\n",
        "        results = self.vector_store.similarity_search_with_relevance_scores(\n",
        "            question, k=k\n",
        "        )\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        filtered_results = [doc for doc, score in results if score >= confidence_threshold]\n",
        "\n",
        "        print(boxen(f\"Found {len(filtered_results)} relevant chunks above threshold {confidence_threshold}\", title=\">>> Context\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Load sample ArXiv PDFs\n",
        "print(boxen(\"Initializing ArXiv processor with sample papers...\", title=\">>> Initialization\", color=\"cyan\", padding=1))\n",
        "arxiv_processor = ArXivProcessor()\n",
        "arxiv_processor.load_and_process([\n",
        "    \"https://arxiv.org/pdf/2504.10412\"   # LLM research paper\n",
        "])\n",
        "print(boxen(\"ArXiv processor initialized!\", title=\">>> Status\", color=\"green\", padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaD77BupxMv"
      },
      "source": [
        "# ArXiv Retrieval Node Documentation\n",
        "\n",
        "## Overview\n",
        "The `arxiv_retrieval_node` function serves as a retrieval component in an agent-based system, fetching relevant scientific information from ArXiv papers based on user queries.\n",
        "\n",
        "## Function Signature\n",
        "`arxiv_retrieval_node(state: AgentState) -> dict`\n",
        "\n",
        "## Parameters\n",
        "- `state`: An AgentState object containing the current conversation state, including:\n",
        " - `question`: The user's query to search for in ArXiv papers\n",
        "\n",
        "## Functionality\n",
        "The function:\n",
        "1. Extracts the user's question from the input state\n",
        "2. Calls the `arxiv_processor.retrieve()` method to find relevant document chunks\n",
        "3. Uses a reduced confidence threshold (0.5) compared to the default (0.75) to improve recall\n",
        "4. Returns the retrieved documents for further processing\n",
        "\n",
        "## Return Value\n",
        "Returns a dictionary with:\n",
        "- `arxiv_results`: A list of document chunks from ArXiv papers relevant to the user's question\n",
        "\n",
        "## Integration Notes\n",
        "- This function is designed to be used as a node in an agent workflow\n",
        "- The reduced confidence threshold ensures more potential matches are returned, prioritizing recall over precision\n",
        "- The retrieved documents can be used by subsequent nodes for answering the user's question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "7QI0RXSbdvCv"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def arxiv_retrieval_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant information from ArXiv papers based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with arxiv_results\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents from ArXiv\n",
        "    relevant_docs = arxiv_processor.retrieve(\n",
        "        question=state[\"question\"],\n",
        "        confidence_threshold=0.5  # Adjusted threshold for better recall\n",
        "    )\n",
        "\n",
        "    # Check if we found enough relevant content\n",
        "    return {\"arxiv_results\": relevant_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqO1XREiD7z"
      },
      "source": [
        "## 5. Web Search Node Implementation\n",
        "\n",
        "The **Web Search Node** uses the **Tavily API** to search the web when **ArXiv papers** don't have the answer.\n",
        "\n",
        "- **Optimizes** the search query\n",
        "- **Filters and processes** results\n",
        "- **Ensures** proper attribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "_WsFmnmpd3CI"
      },
      "outputs": [],
      "source": [
        "web_searcher = TavilyClient()\n",
        "def web_search_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Searches the web for information using the Tavily API.\n",
        "    \"\"\"\n",
        "    # Include academic domains to improve search quality\n",
        "    academic_domains = [\"arxiv.org\", \"scholar.google.com\", \"researchgate.net\", \"edu\"]\n",
        "\n",
        "    # Get search results with answer\n",
        "    search_response = web_searcher.search(\n",
        "        query=state[\"question\"],\n",
        "        max_results=5,\n",
        "        include_domains=academic_domains,\n",
        "        search_depth=\"advanced\",  # Use advanced search for better results\n",
        "        include_answer=True  # Request direct answer\n",
        "    )\n",
        "\n",
        "    # If we have a direct answer, use it\n",
        "    if search_response.get(\"answer\"):\n",
        "        return {\n",
        "            \"web_results\": search_response.get(\"results\", []),\n",
        "            \"direct_answer\": search_response[\"answer\"]\n",
        "        }\n",
        "\n",
        "    return {\"web_results\": search_response.get(\"results\", [])}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoo-tCciKHn"
      },
      "source": [
        "# Function Documentation: `synthesize_answer_node`\n",
        "\n",
        "## Overview\n",
        "The `synthesize_answer_node` function is a key component in a LangChain-based conversational agent. It is responsible for generating a comprehensive answer based on either scientific research papers (from ArXiv) or web search results (via Tavily). The generated response is contextual, well-structured, and strictly grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To synthesize a high-quality, structured, and citation-backed answer from the information retrieved during the conversational flow â€” either from ArXiv research papers or real-time web search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- **state (AgentState)**:  \n",
        "  A dictionary representing the current state of the agent, which includes:\n",
        "  - `question`: The user's query.\n",
        "  - `arxiv_results`: A list of research paper excerpts (if available).\n",
        "  - `web_results`: A list of web search results (used when no ArXiv data is present).\n",
        "  - `conversation_history`: Context from previous exchanges to maintain continuity.\n",
        "\n",
        "---\n",
        "\n",
        "## Logic Flow\n",
        "\n",
        "1. **Source Determination**:  \n",
        "   The function first checks whether ArXiv results are available. If so, it uses them; otherwise, it falls back to web search results.\n",
        "\n",
        "2. **Prompt Construction**:  \n",
        "   A custom `prompt_template` is built depending on the data source. Each template includes:\n",
        "   - The original question.\n",
        "   - Retrieved content (formatted accordingly).\n",
        "   - Prior conversation context.\n",
        "   - Explicit instructions to ensure grounded, factual, and well-structured responses.\n",
        "\n",
        "3. **Model Invocation**:  \n",
        "   - Uses `ChatOpenAI` (specifically `gpt-4o-mini`) for advanced reasoning and response generation.\n",
        "   - Combines the prompt and model into a LangChain chain using `ChatPromptTemplate` and `StrOutputParser`.\n",
        "\n",
        "4. **Response Handling**:  \n",
        "   - If web results were used, the function appends a list of source URLs at the end of the response.\n",
        "   - If ArXiv sources were used, inline citations in the format `(Author et al., Page X)` are expected.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "- Returns a dictionary with a single key:  \n",
        "  - `answer`: A fully formatted, cited response derived from either research papers or search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Characteristics\n",
        "\n",
        "- **Grounded Output**: The model is instructed not to hallucinate or invent facts.\n",
        "- **Citations Included**: Adds credibility and traceability via inline citations or URL references.\n",
        "- **Context-Aware**: Maintains conversation context to provide coherent multi-turn interactions.\n",
        "- **Readable Format**: Uses markdown elements such as headers, bullet points, and bold text for readability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "3h2iGpE9d8VM"
      },
      "outputs": [],
      "source": [
        "def synthesize_answer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Synthesizes a comprehensive answer from retrieved information.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing question and retrieved information\n",
        "\n",
        "    Returns:\n",
        "        Updated state with answer\n",
        "    \"\"\"\n",
        "    # If we have a direct answer from web search, use it\n",
        "    if state.get(\"direct_answer\"):\n",
        "        answer_content = state[\"direct_answer\"]\n",
        "        source_type = \"Web Search Results\"\n",
        "    else:\n",
        "        # Determine which source to use for synthesis\n",
        "        if state[\"arxiv_results\"] and len(state[\"arxiv_results\"]) > 0:\n",
        "            # Using ArXiv research papers\n",
        "            sources = \"\\n\\n\".join([\n",
        "                f\"--- Document: {d.metadata.get('source', 'Unknown')} (Page {d.metadata.get('page', 'Unknown')}) ---\\n{d.page_content}\"\n",
        "                for d in state[\"arxiv_results\"]\n",
        "            ])\n",
        "            displayed_sources = sources\n",
        "            source_type = \"ArXiv Papers\"\n",
        "\n",
        "            prompt_template = \"\"\"\n",
        "            You are a knowledgeable research assistant specializing in mathematical theory and scientific literature analysis.\n",
        "\n",
        "            Your goal is to generate clean, formatted responses to user questions based solely on the provided ArXiv sources.\n",
        "\n",
        "            ---\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Relevant Extracts from ArXiv Papers:\n",
        "            {sources}\n",
        "\n",
        "            Conversation History:\n",
        "            {conversation_history}\n",
        "\n",
        "            ---\n",
        "\n",
        "            Instructions for Synthesizing the Answer:\n",
        "\n",
        "            1. Read the extracts thoroughly and understand the concepts.\n",
        "            2. Answer the question comprehensively using only the provided context.\n",
        "            3. Organize the response into the following markdown sections (if applicable):\n",
        "              - Summary\n",
        "              - Key Concepts\n",
        "              - Theoretical Results\n",
        "              - Implications / Applications\n",
        "            4. Cite from the paper in the format: (Author et al., Page X). If page number is unknown, write: (Author et al.).\n",
        "            6. Avoid repetition, excessive formal tone, or generic commentary. Be clear and concise.**\n",
        "            7. If the provided text lacks enough detail to answer, state it clearly and suggest what additional info is needed.\n",
        "\n",
        "            ---\n",
        "\n",
        "            Now, write a well-structured, markdown-formatted answer to the question and it should be in a readable format as well.\n",
        "            \"\"\"\n",
        "        else:\n",
        "            # Using web search results\n",
        "            sources = \"\\n\\n\".join([\n",
        "                f\"--- Source {i+1}: {res['title']} ---\\n{res['content']}\"\n",
        "                for i, res in enumerate(state[\"web_results\"] or [])\n",
        "            ])\n",
        "            displayed_sources = sources\n",
        "            source_type = \"Web Search Results\"\n",
        "\n",
        "            prompt_template = \"\"\"\n",
        "            You are a knowledgeable research assistant providing accurate information based on web search results.\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Here are relevant web search results:\n",
        "            {sources}\n",
        "\n",
        "            Conversation History:\n",
        "            {conversation_history}\n",
        "\n",
        "            Instructions:\n",
        "            1. Synthesize a comprehensive answer using ONLY the information provided above.\n",
        "            2. Cite sources using [1], [2], etc. corresponding to the source numbers above.\n",
        "            3. If the search results don't contain sufficient information, acknowledge the limitations.\n",
        "            4. DO NOT make up information not present in the sources.\n",
        "            5. Include only facts supported by the sources.\n",
        "\n",
        "            Your answer:\n",
        "            \"\"\"\n",
        "\n",
        "        # Print retrieval information\n",
        "        print(f\"\\n=== Retrieved chunks from {source_type} ===\")\n",
        "        print(displayed_sources)\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Create the prompt\n",
        "        synthesis_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "        # Use a more capable model for synthesis\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "        chain = synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "        # Generate the answer\n",
        "        response = chain.invoke({\n",
        "            \"question\": state[\"question\"],\n",
        "            \"sources\": sources,\n",
        "            \"conversation_history\": state[\"conversation_history\"]\n",
        "        })\n",
        "\n",
        "        # Add source citations for web results\n",
        "        if state.get(\"web_results\") and not state.get(\"arxiv_results\"):\n",
        "            answer_content = response\n",
        "\n",
        "            # Add URL references at the end\n",
        "            url_citations = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "                f\"[{i+1}] {res['url']}\"\n",
        "                for i, res in enumerate(state[\"web_results\"] or [])\n",
        "            ])\n",
        "\n",
        "            answer_content += url_citations\n",
        "        else:\n",
        "            answer_content = response\n",
        "\n",
        "    # Using markdown and plain text for better readability\n",
        "    formatted_output = f\"\"\"\n",
        "## Context\n",
        "**Question:** {state[\"question\"]}\n",
        "**Source:** {source_type}\n",
        "\n",
        "## Response\n",
        "{answer_content}\n",
        "\"\"\"\n",
        "\n",
        "    return {\"answer\": formatted_output}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSZSMQBiaMC"
      },
      "source": [
        "## 7. Conversation Memory Node\n",
        "\n",
        "This node **manages conversation history** to provide context for **multi-turn interactions**.\n",
        "\n",
        "- **Stores** previous Q&A\n",
        "- **Updates** the state with the current interaction\n",
        "- **Maintains** a sliding window of relevant history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Pgefbaa_eAfZ"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Initialize conversation memory\n",
        "\n",
        "def update_memory_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Updates the conversation memory with the current Q&A pair.\n",
        "\n",
        "    Args:\n",
        "        state: Current state with question and answer\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new conversation_history\n",
        "    \"\"\"\n",
        "    # Save the current interaction to memory\n",
        "    memory = state['memory']\n",
        "    memory.save_context(\n",
        "        {\"question\": state[\"question\"]},\n",
        "        {\"answer\": state[\"answer\"]}\n",
        "    )\n",
        "\n",
        "    # Return the updated state\n",
        "    return {\"conversation_history\": memory.load_memory_variables({}).get(\"history\", \"\")}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxKEeu64iglN"
      },
      "source": [
        "# Workflow State Graph Setup\n",
        "\n",
        "## Overview\n",
        "This section sets up the **LangGraph state machine** for managing the conversational agentâ€™s workflow. It defines how user queries are processed step-by-step using modular nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To create a graph-based control flow that determines how the agent processes input, performs retrieval, synthesizes responses, updates memory, and eventually ends the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Workflow Initialization**\n",
        "- A new `StateGraph` is initialized with the `AgentState` type, defining the structure of the workflow.\n",
        "\n",
        "### 2. **Node Definitions**\n",
        "The graph is composed of several functional nodes, each responsible for a specific task:\n",
        "- **router**: Determines whether to fetch data from the web or ArXiv.\n",
        "- **arxiv_retrieval**: Retrieves relevant research papers from ArXiv.\n",
        "- **web_search**: Retrieves web results via Tavily.\n",
        "- **synthesize**: Synthesizes a final answer from the retrieved information.\n",
        "- **update_memory**: Stores the interaction context for future turns.\n",
        "\n",
        "### 3. **Entry Point**\n",
        "- The `router` node is set as the initial entry point for the graph, meaning every workflow starts with routing logic.\n",
        "\n",
        "### 4. **Conditional Routing**\n",
        "- A conditional edge is established from `router` based on the `\"next\"` field in the state:\n",
        "  - If `\"next\"` is `\"web_search\"`, it routes to the `web_search` node.\n",
        "  - If `\"next\"` is `\"arxiv_retrieval\"`, it routes to the `arxiv_retrieval` node.\n",
        "\n",
        "### 5. **Workflow Sequence**\n",
        "The following fixed transitions define the remainder of the workflow:\n",
        "- From either `web_search` or `arxiv_retrieval` â†’ go to `synthesize`\n",
        "- From `synthesize` â†’ go to `update_memory`\n",
        "- From `update_memory` â†’ reach `END` (completion of the flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "BRYbjFhHeEZt"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Create the workflow state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes to the graph\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"arxiv_retrieval\", arxiv_retrieval_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"synthesize\", synthesize_answer_node)\n",
        "workflow.add_node(\"update_memory\", update_memory_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# Define conditional edges from router\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state[\"next\"],\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"arxiv_retrieval\": \"arxiv_retrieval\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define rest of the edges\n",
        "workflow.add_edge(\"arxiv_retrieval\", \"synthesize\")\n",
        "workflow.add_edge(\"web_search\", \"synthesize\")\n",
        "workflow.add_edge(\"synthesize\", \"update_memory\")\n",
        "workflow.add_edge(\"update_memory\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4TP3xSioHc"
      },
      "source": [
        "## 9. Testing the System\n",
        "\n",
        "Let's **test our system** with different types of questions:\n",
        "\n",
        "- **Questions answerable** from ArXiv papers\n",
        "- **Questions requiring** web search\n",
        "- **Follow-up questions** to test memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(return_messages=False, output_key=\"answer\", input_key=\"question\")\n",
        "initial_state = {\n",
        "    \"question\": None,\n",
        "    \"arxiv_results\": None,\n",
        "    \"web_results\": None,\n",
        "    \"answer\": \"\",\n",
        "    \"conversation_history\": memory.load_memory_variables({}).get(\"history\", \"\"),\n",
        "    \"memory\": memory\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "gFoxsXLWeHSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask(app, question: str, state: AgentState):\n",
        "    \"\"\"\n",
        "    Ask a question to the agentic RAG system.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "\n",
        "    Returns:\n",
        "        The system's answer\n",
        "    \"\"\"\n",
        "    # Initialize the state\n",
        "\n",
        "    # Invoke the workflow\n",
        "    state['question'] = question\n",
        "    result = app.invoke(state)\n",
        "    # Print the response details with pyboxen\n",
        "    print(boxen(f\"Question: {result['question']}\", title=\">>> Question\", color=\"blue\", padding=1))\n",
        "\n",
        "    if result[\"arxiv_results\"]:\n",
        "        arxiv_count = len(result[\"arxiv_results\"])\n",
        "        print(boxen(f\"Found {arxiv_count} ArXiv results\", title=\">>> ArXiv Results\", color=\"magenta\", padding=1))\n",
        "    elif result[\"web_results\"]:\n",
        "        web_count = len(result[\"web_results\"])\n",
        "        print(boxen(f\"Found {web_count} Web results\", title=\">>> Web Results\", color=\"magenta\", padding=1))\n",
        "    else:\n",
        "        print(boxen(\"No results found\", title=\">>> Results\", color=\"red\", padding=1))\n",
        "\n",
        "    print(boxen(result[\"answer\"], title=\">>> Answer\", color=\"green\", padding=1))\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0Ps7Nw1exMl",
        "outputId": "a0e5a80d-2c9e-42e6-f085-678b3931c50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Router decision: arxiv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mâ•­â”€\u001b[0m\u001b[33m >>> Context \u001b[0m\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[33mâ”€â•®\u001b[0m                                                                \n",
            "\u001b[33mâ”‚\u001b[0m                                                 \u001b[33mâ”‚\u001b[0m                                                                \n",
            "\u001b[33mâ”‚\u001b[0m   Found 5 relevant chunks above threshold 0.5   \u001b[33mâ”‚\u001b[0m                                                                \n",
            "\u001b[33mâ”‚\u001b[0m                                                 \u001b[33mâ”‚\u001b[0m                                                                \n",
            "\u001b[33mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                \n",
            "\n",
            "\n",
            "=== Retrieved chunks from ArXiv Papers ===\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "peek ahead (real-time refactoring bots). Bar charts compare\n",
            "precision, tables list metrics, and AST graphs show GNN\n",
            "magic. It is a step toward codebases that do not fight back.\n",
            "II. THEORETICAL BACKGROUND\n",
            "Code refactoring is not newâ€”Fowlerâ€™s 1999 book codified it:\n",
            "extract methods, reduce duplication, tame complexity [8].\n",
            "Maintainability hinges on metrics: cyclomatic complexity\n",
            "(paths through codeâ€”10â€™s a red flag), coupling (module\n",
            "dependenciesâ€”5+ screams trouble), and cohesion (how tight a\n",
            "moduleâ€™s purpose is) [9]. High complexityâ€”like a 50-path\n",
            "functionâ€”means bugs hide easier; tight couplingâ€”like twenty\n",
            "cross-module callsâ€”means changes ripple hard. ASTs\n",
            "formalize this. A Python line, if x > 0: y = x, becomes a tree:\n",
            "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance\n",
            "Software Maintainability\n",
            "Gopichand Bandarupalli1\n",
            "1ai.ml.research.articles@gmail.com\n",
            "1Professional M.B.A., Campbellsville university, Texas, USA\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mâ•­â”€\u001b[0m\u001b[34m >>> Question \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m                                                                     \n",
            "\u001b[34mâ”‚\u001b[0m                                            \u001b[34mâ”‚\u001b[0m                                                                     \n",
            "\u001b[34mâ”‚\u001b[0m   Question: Explain Software Refactoring   \u001b[34mâ”‚\u001b[0m                                                                     \n",
            "\u001b[34mâ”‚\u001b[0m                                            \u001b[34mâ”‚\u001b[0m                                                                     \n",
            "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                     \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mâ•­â”€\u001b[0m\u001b[35m >>> ArXiv Results \u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35mâ”€â•®\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m                           \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m   Found 5 ArXiv results   \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m                           \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                                      \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mâ•­â”€\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Context                                                                                                    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   **Question:** Explain Software Refactoring                                                                    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   **Source:** ArXiv Papers                                                                                      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Response                                                                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   # Software Refactoring                                                                                        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Summary                                                                                                    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   Software refactoring is a crucial aspect of software engineering that involves modifying and restructuring    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   code to enhance its readability, maintainability, and clarity without altering its external behavior. This    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   practice is increasingly important as software complexity grows, posing challenges for developers who often   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   spend a significant portion of their time dealing with difficult codebases.                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Key Concepts                                                                                               \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Definition**: Refactoring is the process of revamping code to make it cleaner, more understandable, and   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   easier to maintain while ensuring there are no functional changes in the software.                            \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Complexity Challenges**: Many developers reportedly spend about 30% of their time managing complex code   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   issues. Increased complexity can lead to higher bug rates (up to 25%) and slow down the rollout of new        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   features by 40% (Author et al.).                                                                              \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Traditional Tools**: Tools such as SonarQube or Checkstyle help identify code issues but often focus on   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   rigid metrics that might not capture the broader context of code maintainability (Author et al.).             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Theoretical Results                                                                                        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   The theoretical basis for software refactoring is rooted in principles established by experts in the field.   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   Notably, Martin Fowler's work in 1999 laid down key methods such as:                                          \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Extract Methods**: Breaking down large functions into smaller, more manageable ones.                      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Reduce Duplication**: Minimizing repeated code to streamline maintenance.                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Tame Complexity**: Managing the intricacies in code logic to enhance clarity (Author et al.).             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   Refactoring metrics include:                                                                                  \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Cyclomatic Complexity**: Evaluates potential paths through the code; a complexity of over 10 indicates    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   a need for refactoring.                                                                                       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Coupling**: Assesses the dependency between modules; a high number (5+) indicates greater risk when       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   changes are made.                                                                                             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Cohesion**: Measures how closely related the functions within a module are. High cohesion is desirable    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   for maintainability (Author et al.).                                                                          \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Implications / Applications                                                                                \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   The integration of advanced techniques, such as AI-driven solutions using Graph Neural Networks (GNN),        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   offers a scalable approach to code refactoring that could significantly enhance software maintainability.     \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   This promising avenue aims to automate and optimize the refactoring process, easing the burden on             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   developers and potentially leading to cleaner, more resilient codebases (Author et al.).                      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   In summary, software refactoring is a fundamental practice that not only improves code quality but also       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   impacts the overall efficiency of software development. Continued exploration of innovative methods is        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   essential for advancing this critical aspect of software engineering.                                         \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with a question about quantum computing (should use ArXiv)\n",
        "updated_state = ask(app, \"Explain Software Refactoring\", initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7keGgbrWgqy9",
        "outputId": "faca9a57-6d45-42a0-c10c-44595bbaf3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Router decision: web\n",
            "\n",
            "=== Retrieved chunks from ArXiv Papers ===\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineeringâ€™s future.  \n",
            "Index Termsâ€” Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringâ€”the art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highâ€”poor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "peek ahead (real-time refactoring bots). Bar charts compare\n",
            "precision, tables list metrics, and AST graphs show GNN\n",
            "magic. It is a step toward codebases that do not fight back.\n",
            "II. THEORETICAL BACKGROUND\n",
            "Code refactoring is not newâ€”Fowlerâ€™s 1999 book codified it:\n",
            "extract methods, reduce duplication, tame complexity [8].\n",
            "Maintainability hinges on metrics: cyclomatic complexity\n",
            "(paths through codeâ€”10â€™s a red flag), coupling (module\n",
            "dependenciesâ€”5+ screams trouble), and cohesion (how tight a\n",
            "moduleâ€™s purpose is) [9]. High complexityâ€”like a 50-path\n",
            "functionâ€”means bugs hide easier; tight couplingâ€”like twenty\n",
            "cross-module callsâ€”means changes ripple hard. ASTs\n",
            "formalize this. A Python line, if x > 0: y = x, becomes a tree:\n",
            "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance\n",
            "Software Maintainability\n",
            "Gopichand Bandarupalli1\n",
            "1ai.ml.research.articles@gmail.com\n",
            "1Professional M.B.A., Campbellsville university, Texas, USA\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mâ•­â”€\u001b[0m\u001b[34m >>> Question \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m                                                    \n",
            "\u001b[34mâ”‚\u001b[0m                                                             \u001b[34mâ”‚\u001b[0m                                                    \n",
            "\u001b[34mâ”‚\u001b[0m   Question: What is Crew Ai ? and can it help me refactor   \u001b[34mâ”‚\u001b[0m                                                    \n",
            "\u001b[34mâ”‚\u001b[0m                                                             \u001b[34mâ”‚\u001b[0m                                                    \n",
            "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                    \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mâ•­â”€\u001b[0m\u001b[35m >>> ArXiv Results \u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35mâ”€â•®\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m                           \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m   Found 5 ArXiv results   \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ”‚\u001b[0m                           \u001b[35mâ”‚\u001b[0m                                                                                      \n",
            "\u001b[35mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m                                                                                      \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mâ•­â”€\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Context                                                                                                    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   **Question:** What is Crew Ai ? and can it help me refactor                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   **Source:** ArXiv Papers                                                                                      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Response                                                                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ## Crew AI and Its Role in Code Refactoring                                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ### Summary                                                                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   Crew AI refers to an AI-driven tool aimed at enhancing software maintainability by providing a path to        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   cleaner codebases. By leveraging advanced techniques such as Graph Neural Networks (GNN), Crew AI offers      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   innovative ways to assist developers in refactoring their codeâ€”resulting in improved readability,             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   maintainability, and overall software quality without changing the underlying functionality.                  \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ### Key Concepts                                                                                              \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Purpose**: The primary goal of Crew AI is to facilitate software refactoring, which involves modifying    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   the code structure while preserving behavior. This is crucial for managing complexities that often lead to    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   maintenance challenges.                                                                                       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Current Industry Challenges**: Developers spend significant time (approximately 30%) grappling with       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   complex code, leading to increased bugs and slower feature rollouts when codebases are poorly maintained      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   (Author et al.).                                                                                              \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Traditional Tools**: Existing tools like SonarQube offer some level of assistance but often utilize       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   rigid metrics that fail to assess the overall health of the codebase adequately (Author et al.).              \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ### Theoretical Results                                                                                       \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   The theoretical underpinnings of Crew AI and similar tools draw on well-established principles of software    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   refactoring. Key strategies include:                                                                          \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Metrics of Maintainability**:                                                                             \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m     - **Cyclomatic Complexity**: This measures the number of paths through a codebase. High values are          \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   indicative of potential maintenance issues (Author et al.).                                                   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m     - **Coupling**: This assesses module dependencies, where high coupling might complicate changes across      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   modules (Author et al.).                                                                                      \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m     - **Cohesion**: Indicates how well-focused a module's responsibilities are, with high cohesion being        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ideal for maintainability (Author et al.).                                                                    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   - **Advanced Techniques**: The integration of GNN with refactoring practices represents an opportunity to     \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   automate the identification and improvement of complex code patterns.                                         \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   ### Implications / Applications                                                                               \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   Crew AI's application promises to significantly improve the efficacy of code refactoring processes,           \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   offering real-time insights and visualizations to help developers quickly address code quality issues. By     \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   optimizing refactoring efforts, it not only enhances code maintainability but also reduces the risk of        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   introducing bugs, thus streamlining the development process (Author et al.). With its scalable, AI-driven     \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   approach, Crew AI represents a forward-thinking solution for the increasingly complex landscape of software   \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   engineering, enhancing the overall quality and maintainability of codebases.                                  \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   In conclusion, Crew AI stands to revolutionize how code refactoring is approached in software engineering,    \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m   making it a valuable asset for developers aiming to maintain clean and efficient code.                        \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
            "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with a question about recent developments (should use web)\n",
        "updated_state = ask(app, \"What is Crew Ai ? and can it help me refactor\", updated_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TtHrxeMo1mr"
      },
      "source": [
        "## ðŸŽ“ Conclusion\n",
        "\n",
        "The Agentic RAG System with ArXiv + Web Fallback represents a powerful approach to information retrieval and synthesis, combining the best of both academic and real-time knowledge sources. By intelligently routing queries and maintaining conversation context, it provides:\n",
        "\n",
        "- **Comprehensive Answers**: Leveraging both academic papers and current web information\n",
        "- **Proper Attribution**: Ensuring all sources are properly cited\n",
        "- **Contextual Understanding**: Maintaining conversation history for coherent interactions\n",
        "- **Flexible Knowledge Access**: Adapting to different types of queries and information needs\n",
        "\n",
        "This system is particularly valuable for:\n",
        "- Researchers seeking both theoretical foundations and practical applications\n",
        "- Developers looking for up-to-date technical information\n",
        "- Students and professionals needing comprehensive, well-sourced answers\n",
        "- Anyone requiring a balance between academic rigor and current information\n",
        "\n",
        "The modular architecture and use of LangGraph make it easy to extend and adapt the system for specific use cases or additional knowledge sources."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
